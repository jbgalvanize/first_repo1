<html>

<head>
<title>Classification and Regression Trees</title>
</head>

<body>

<b>Classification and Regression Trees</b>
<br></br>
<b>Classification Trees</b>
<br></br>
<p>In this lab, we will be showing how decision trees can make predictions for both categorical and quantitative response variable cases.  In the case of a categorical response, we call the model a classification tree.  In the case of a quantitative response, we call our model a regression tree.  First, let's look at a classification tree example.</p>

<p>Here, we are loading the necessary libraries and attaching our dataset.</p>

<!--begin.rcode
library(tree)
library(ISLR)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(C50)
attach(Carseats)
end.rcode-->

<p>We can take a quick look at our dataset of interest.  In a real analysis, you would want to take a closer look at the univariate, bivariate, etc. relationships that exist using summary statistics and visual methods to explore.  Here, I just want to give you an idea of the type of data we will be working with for our classification example.</p>

<!--begin.rcode
head(Carseats)
dim(Carseats)
str(Carseats)
?Carseats
end.rcode-->

<p>We might want to predict whether a company will have 'low', 'medium', or 'high' sales.  In order to create our new variable that we would like to predict, I use the <b>cut()</b> function below. I then rename the levels of our variable. </p>

<!--begin.rcode
Carseats$Sales1 = cut(Sales,3) 
levels(Carseats$Sales1) = c("low", "med", "high") 
end.rcode-->

<p>In order to check how well our tree is able to predict, I have created training and validation datasets.</p>

<!--begin.rcode
set.seed(1)
train = sample(1:nrow(Carseats), nrow(Carseats)/2) 
train_dat = Carseats[train,] 
val_dat = Carseats[-train,] 
end.rcode-->

<p>We can now fit our classification tree to the training data, and we can take a look at how it chooses to split our data into particular categories. The 'pretty' argument determines the font type used on your decision tree.</p>

<!--begin.rcode fig.width=12, fig.height=6
mod = tree(Sales1~CompPrice+Income+Advertising+Population+Price+ShelveLoc+Age+Education+Urban+US, train_dat) 
plot(mod) 
text(mod, pretty=0) 
end.rcode-->

<p>We can learn a bit more about our model by printing it or looking at a summary.</p>

<!--begin.rcode
#mod 
summary(mod)
end.rcode-->

<p>We might want to use cross-validation to assure that our tree isn't over-fitting the data.  We can do this using the <b>cv.tree()</b> function.  The default is 10-fold, which means we split the dataset into 10 equal sized parts.  We fit our model to 9 of the 10 parts and predict for the last set.  We then change to a new validation set.  Finally, we average the predictions from each of the k-folds.  You can look at the k-folds algorithm described <a href ="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">here</a>.  The <b>names()</b> function provides the options internal to our model.</p>

<!--begin.rcode
cv_mod = cv.tree(mod, FUN=prune.misclass) 
names(cv_mod)
end.rcode-->

<p>We apply our results to the <b>prune.tree()</b> function. We could choose the number of ending nodes in our tree based on minimizing the deviance using the misclassification method - that is, we are minimizing the number of misclassifications we are making.  Below, I am pruning the classification tree based on the choosing the number of ending nodes where the deviance is smallest.</p>

<!--begin.rcode fig.width=12, fig.height=6
prune_mod = prune.tree(mod, best = cv_mod$size[which.min(cv_mod$dev)]) 
plot(prune_mod)
text(prune_mod)
end.rcode-->

<p>There are two additional libraries that can be used to fit decision trees, which use the <b>rpart()</b> and <b>C5.0()</b> functions.  These decision trees are both fit below.  Unfortunately, it is not easy to plot the tree using the final function.  For more details on the below functions please see the documentation.</p>

<!--begin.rcode fig.width=12, fig.height=6
mod1 = rpart(Sales1~CompPrice+Income+Advertising+Population+Price+ShelveLoc+Age+Education+Urban+US, train_dat) 
rpart.plot(mod1, type=3, extra=101,fallen.leaves=TRUE)

c50_mod = C5.0(train_dat[,c(2:11)], train_dat$Sales1)
#summary(c50_mod)
end.rcode-->

<p>The way a model chooses to split is on the basis of <b>entropy</b>, <b>gini</b>, and/or <b>misclassification</b> rates.  Looking through the documentation shows which method is used to determine splits.  The gini method is very popular.  You can see how this method is completed in a small example <a href="https://www.youtube.com/watch?v=Zze7SKuz9QQ">here</a>.  Later methods that are combined with decision trees will help us optimize regardless of the technique you use for splitting.</p>

<p>We can check how well each method above predicts by constructing a confusion matrix of the results from each model.  Here, I have made predictions from each model:</p>

<!--begin.rcode
pred_prune_mod = predict(prune_mod, val_dat, type = "class")
pred_rpart_mod = predict(mod1, val_dat, type = "class")
pred_c50_mod = predict(c50_mod, val_dat, type = "class")
pred_mod = predict(mod, val_dat, type = "class")
end.rcode-->

<p>Then we can look at the confusion matrices from each method and the misclassification rates.</p>

<!--begin.rcode
table(pred_prune_mod, val_dat$Sales1)
table(pred_rpart_mod, val_dat$Sales1)
table(pred_c50_mod, val_dat$Sales1)
table(pred_mod, val_dat$Sales1)

#mis-classification rates
mean(pred_prune_mod != val_dat$Sales1)
mean(pred_rpart_mod != val_dat$Sales1)
mean(pred_c50_mod != val_dat$Sales1)
mean(pred_mod != val_dat$Sales1)
end.rcode-->

<b>Regression Tree</b>
<br></br>
<p>Now, we will look at a regression tree example.  Here, I will use the Boston dataset.</p>

<!--begin.rcode 
library(MASS)
library(ggplot2)
attach(Boston)
?Boston
head(Boston)
dim(Boston)
str(Boston)
end.rcode-->

<p>Here, we can take a quick look at the univariate and bivariate relationships visually.  I have provided an easy way to look at multiple univariate plots at the same time.  You may choose not to run the qplot() functions.  I have commented out all of the plots for the ease of reading this document.</p>

<!--begin.rcode
#par(mfrow=c(3,3))
#hist(medv);hist(lstat);hist(black)
#hist(crim);hist(age);hist(tax)
#hist(rm);hist(ptratio);hist(nox)

#qplot(age,medv);qplot(lstat,medv);qplot(crim,medv)
#qplot(rm,medv);qplot(ptratio,medv);qplot(nox,medv)
#qplot(zn,medv);qplot(black,medv);qplot(tax,medv)
end.rcode-->

<p>Again, let's split our dataset into training and validation.</p>

<!--begin.rcode
train = sample(1:nrow(Boston), nrow(Boston)/2) 
train_dat = Boston[train,]
val_dat = Boston[-train,]
end.rcode-->

<p>Now, we can go through a similar process to the above to fit a tree in a regression setting.  Here, we usually try to minimize the MSE (which is the deviance) on the validation set during cross-validation.  Because we did the same process above, I simply left comments in the code to assist as you work through.</p>

<!--begin.rcode
mod = tree(medv~., train_dat) #create tree using all of the predictors
#mod #You can see how many observations go into each part of the split
plot(mod) #Plot the tree model
text(mod, pretty=0) #Add text for the tree model
summary(mod) #Summary that corresponds to tree
end.rcode-->



<!--begin.rcode
pred_val = predict(mod, val_dat)
mean((pred_val-val_dat$medv)^2)

#Plot predicted vs actual or plot the 
plot(pred_val, val_dat$medv)
plot(pred_val,(pred_val-val_dat$medv))
abline(h=0, col=2)

#We could find out if we need to trim the tree by using cross-validation
cv_mod = cv.tree(mod) #default is 10 fold, you can change the number of folds using k=(number) as an argument
names(cv_mod)

plot(cv_mod$size, cv_mod$dev, type = 'b') 
cv_mod$size[which.min(cv_mod$dev)] 

prune_mod = prune.tree(mod, best = 4) #Use prune.tree where there are only 4 final predictions
plot(prune_mod)
text(prune_mod)

#Compare MSE to above
prune_pred_val = predict(prune_mod, val_dat)
mean((prune_pred_val-val_dat$medv)^2)
end.rcode-->
<p>For this case, our cross-validation model did worse.</p>
</body>
</html>
